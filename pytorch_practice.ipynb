{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dfcaee",
   "metadata": {},
   "source": [
    "A3. seeing models as computational graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a01c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94935074",
   "metadata": {},
   "source": [
    "AUTOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb20e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f51625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a078008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063ae57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4153aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625a58e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2879, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recompute y and z before backward to restore the computation graph\n",
    "y = x**2\n",
    "z = torch.sin(y)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89b2c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19eea2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash Malik\\AppData\\Local\\Temp\\ipykernel_46716\\486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818b2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs \n",
    "\n",
    "x = torch.tensor(6.7) # input feature \n",
    "y = torch.tensor(0.0) # true label \n",
    "\n",
    "\n",
    "w = torch.tensor(1.0) # weight\n",
    "b = torch.tensor(0.0) # bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af953064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross entropy loss for scalar\n",
    "def binary_cross_entropy(prediction, target):\n",
    "    epsilon = 1e-7 # small constant to avoid log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon) # clamp is used to limit the values within a range\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92d40687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass \n",
    "z = w * x + b # linear transformation \n",
    "y_pred = torch.sigmoid(z) # activation function \n",
    "\n",
    "# compute binary cross entrpy loss\n",
    "loss = binary_cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e30a8a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded168f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives:\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
    "\n",
    "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "\n",
    "# 3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x  # dz/dw = x\n",
    "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b450e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ae263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(6.7, requires_grad=True)\n",
    "y = torch.tensor(0.0, requires_grad= True)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True) # weight\n",
    "b = torch.tensor(0.0, requires_grad=True) # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5b92b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "082a111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., requires_grad=True), tensor(0., requires_grad=True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73222116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass \n",
    "z = w * x + b # linear transformation \n",
    "y_pred = torch.sigmoid(z) # activation function \n",
    "\n",
    "# compute binary cross entrpy loss\n",
    "loss = binary_cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8a87d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "712e207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute forward pass to restore computation graph\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = binary_cross_entropy(y_pred, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95b27736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb9aab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0 , 2.0 , 3.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5a1d162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x**2).mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25b8908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21e90dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2818624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearing gradient \n",
    "x = torch.tensor(2.0 , requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c409ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fb891be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "401f070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0c9cabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560914a",
   "metadata": {},
   "source": [
    "## iterator funciton in python . \n",
    "## next function in python . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bdd51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4]\n",
    "my_iterator = iter(my_list)\n",
    "\n",
    "print(next(my_iterator))\n",
    "print(next(my_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04ddb2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "def roll_dice():\n",
    "    return random.randint(1, 6)\n",
    "\n",
    "#create an iterator that calls roll_dice untill it returns 6\n",
    "dice_rolls = iter(roll_dice, 6)\n",
    "\n",
    "for roll in dice_rolls:\n",
    "    print(roll)\n",
    "# this will print random untill 6 is rolled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99655c31",
   "metadata": {},
   "source": [
    "## Understanding the difference between Embeddings layer and Linear layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36036cc",
   "metadata": {},
   "source": [
    "embeddings layers are compuational efficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose we have following 3 training examples \n",
    "# which may represnt token IDs in a LLM context \n",
    "idx = torch.tensor([2,3,1])\n",
    "\n",
    "# The number of rows in the embedding matrix can be determined\n",
    "# by obtaining the largest token ID + 1.\n",
    "# If the highest token ID is 3, then we want 4 rows, for the possible\n",
    "# token IDs 0, 1, 2, 3\n",
    "num_idx = max(idx) + 1 #max(idx) -> max value\n",
    "\n",
    "# the desired embeddings dimension is an hyper parameter\n",
    "out_dim = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd900b",
   "metadata": {},
   "source": [
    "implementing an simple embedding layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b548e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we random seed for reproducibility since\n",
    "# weights in embedding layers are initilaized with \n",
    "# small random values \n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding = torch.nn.Embedding(num_idx, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11225f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n",
       "        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae13dcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ce033fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([2,3,1])\n",
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3fcad",
   "metadata": {},
   "source": [
    "converting tokenIDs into one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89997aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx) #unlike classes like (nn.linear, nn.Conv2D), \n",
    "onehot                                   #funciton in torch.functional do not maintain internal state or learnable parameters like weights and biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1e554",
   "metadata": {},
   "source": [
    "next we initaialize Linear layer to carry out matrix multiplication X.W(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02fde957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2039,  0.0166, -0.2483,  0.1886],\n",
       "        [-0.4260,  0.3665, -0.3634, -0.3975],\n",
       "        [-0.3159,  0.2264, -0.1847,  0.1871],\n",
       "        [-0.4244, -0.3034, -0.1836, -0.0983],\n",
       "        [-0.3814,  0.3274, -0.1179,  0.1605]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# nn.layer(in_features, out_features, bias=True)\n",
    "linear = torch.nn.Linear(num_idx, out_dim, bias=False) #this layer computes y=xW(transpose)+b\n",
    "linear.weight #weight matrix of shape (out_features, in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be00a19",
   "metadata": {},
   "source": [
    "Note that the LINEAR layer in Pytorch is also intialized with small random weights; to directly compare it to the EMBEDDING layer above , we have to use same small random weights, which is why we reassign them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9bf7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66158d",
   "metadata": {},
   "source": [
    "now we can use linear layer on one hot encoded representation of the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f6554c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(onehot.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e32b0",
   "metadata": {},
   "source": [
    "this is exactly same as we got when we used embeding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33fcf7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a99dc",
   "metadata": {},
   "source": [
    "## Nested loops : Pairwise comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e581b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00\n",
      "00:01\n",
      "00:02\n",
      "00:03\n",
      "00:04\n",
      "00:05\n",
      "00:06\n",
      "00:07\n",
      "00:08\n",
      "00:09\n",
      "00:10\n",
      "00:11\n",
      "00:12\n",
      "00:13\n",
      "00:14\n",
      "00:15\n",
      "00:16\n",
      "00:17\n",
      "00:18\n",
      "00:19\n",
      "00:20\n",
      "00:21\n",
      "00:22\n",
      "00:23\n",
      "00:24\n",
      "00:25\n",
      "00:26\n",
      "00:27\n",
      "00:28\n",
      "00:29\n",
      "00:30\n",
      "00:31\n",
      "00:32\n",
      "00:33\n",
      "00:34\n",
      "00:35\n",
      "00:36\n",
      "00:37\n",
      "00:38\n",
      "00:39\n",
      "00:40\n",
      "00:41\n",
      "00:42\n",
      "00:43\n",
      "00:44\n",
      "00:45\n",
      "00:46\n",
      "00:47\n",
      "00:48\n",
      "00:49\n",
      "00:50\n",
      "00:51\n",
      "00:52\n",
      "00:53\n",
      "00:54\n",
      "00:55\n",
      "00:56\n",
      "00:57\n",
      "00:58\n",
      "00:59\n",
      "01:00\n",
      "01:01\n",
      "01:02\n",
      "01:03\n",
      "01:04\n",
      "01:05\n",
      "01:06\n",
      "01:07\n",
      "01:08\n",
      "01:09\n",
      "01:10\n",
      "01:11\n",
      "01:12\n",
      "01:13\n",
      "01:14\n",
      "01:15\n",
      "01:16\n",
      "01:17\n",
      "01:18\n",
      "01:19\n",
      "01:20\n",
      "01:21\n",
      "01:22\n",
      "01:23\n",
      "01:24\n",
      "01:25\n",
      "01:26\n",
      "01:27\n",
      "01:28\n",
      "01:29\n",
      "01:30\n",
      "01:31\n",
      "01:32\n",
      "01:33\n",
      "01:34\n",
      "01:35\n",
      "01:36\n",
      "01:37\n",
      "01:38\n",
      "01:39\n",
      "01:40\n",
      "01:41\n",
      "01:42\n",
      "01:43\n",
      "01:44\n",
      "01:45\n",
      "01:46\n",
      "01:47\n",
      "01:48\n",
      "01:49\n",
      "01:50\n",
      "01:51\n",
      "01:52\n",
      "01:53\n",
      "01:54\n",
      "01:55\n",
      "01:56\n",
      "01:57\n",
      "01:58\n",
      "01:59\n",
      "02:00\n",
      "02:01\n",
      "02:02\n",
      "02:03\n",
      "02:04\n",
      "02:05\n",
      "02:06\n",
      "02:07\n",
      "02:08\n",
      "02:09\n",
      "02:10\n",
      "02:11\n",
      "02:12\n",
      "02:13\n",
      "02:14\n",
      "02:15\n",
      "02:16\n",
      "02:17\n",
      "02:18\n",
      "02:19\n",
      "02:20\n",
      "02:21\n",
      "02:22\n",
      "02:23\n",
      "02:24\n",
      "02:25\n",
      "02:26\n",
      "02:27\n",
      "02:28\n",
      "02:29\n",
      "02:30\n",
      "02:31\n",
      "02:32\n",
      "02:33\n",
      "02:34\n",
      "02:35\n",
      "02:36\n",
      "02:37\n",
      "02:38\n",
      "02:39\n",
      "02:40\n",
      "02:41\n",
      "02:42\n",
      "02:43\n",
      "02:44\n",
      "02:45\n",
      "02:46\n",
      "02:47\n",
      "02:48\n",
      "02:49\n",
      "02:50\n",
      "02:51\n",
      "02:52\n",
      "02:53\n",
      "02:54\n",
      "02:55\n",
      "02:56\n",
      "02:57\n",
      "02:58\n",
      "02:59\n",
      "03:00\n",
      "03:01\n",
      "03:02\n",
      "03:03\n",
      "03:04\n",
      "03:05\n",
      "03:06\n",
      "03:07\n",
      "03:08\n",
      "03:09\n",
      "03:10\n",
      "03:11\n",
      "03:12\n",
      "03:13\n",
      "03:14\n",
      "03:15\n",
      "03:16\n",
      "03:17\n",
      "03:18\n",
      "03:19\n",
      "03:20\n",
      "03:21\n",
      "03:22\n",
      "03:23\n",
      "03:24\n",
      "03:25\n",
      "03:26\n",
      "03:27\n",
      "03:28\n",
      "03:29\n",
      "03:30\n",
      "03:31\n",
      "03:32\n",
      "03:33\n",
      "03:34\n",
      "03:35\n",
      "03:36\n",
      "03:37\n",
      "03:38\n",
      "03:39\n",
      "03:40\n",
      "03:41\n",
      "03:42\n",
      "03:43\n",
      "03:44\n",
      "03:45\n",
      "03:46\n",
      "03:47\n",
      "03:48\n",
      "03:49\n",
      "03:50\n",
      "03:51\n",
      "03:52\n",
      "03:53\n",
      "03:54\n",
      "03:55\n",
      "03:56\n",
      "03:57\n",
      "03:58\n",
      "03:59\n",
      "04:00\n",
      "04:01\n",
      "04:02\n",
      "04:03\n",
      "04:04\n",
      "04:05\n",
      "04:06\n",
      "04:07\n",
      "04:08\n",
      "04:09\n",
      "04:10\n",
      "04:11\n",
      "04:12\n",
      "04:13\n",
      "04:14\n",
      "04:15\n",
      "04:16\n",
      "04:17\n",
      "04:18\n",
      "04:19\n",
      "04:20\n",
      "04:21\n",
      "04:22\n",
      "04:23\n",
      "04:24\n",
      "04:25\n",
      "04:26\n",
      "04:27\n",
      "04:28\n",
      "04:29\n",
      "04:30\n",
      "04:31\n",
      "04:32\n",
      "04:33\n",
      "04:34\n",
      "04:35\n",
      "04:36\n",
      "04:37\n",
      "04:38\n",
      "04:39\n",
      "04:40\n",
      "04:41\n",
      "04:42\n",
      "04:43\n",
      "04:44\n",
      "04:45\n",
      "04:46\n",
      "04:47\n",
      "04:48\n",
      "04:49\n",
      "04:50\n",
      "04:51\n",
      "04:52\n",
      "04:53\n",
      "04:54\n",
      "04:55\n",
      "04:56\n",
      "04:57\n",
      "04:58\n",
      "04:59\n",
      "05:00\n",
      "05:01\n",
      "05:02\n",
      "05:03\n",
      "05:04\n",
      "05:05\n",
      "05:06\n",
      "05:07\n",
      "05:08\n",
      "05:09\n",
      "05:10\n",
      "05:11\n",
      "05:12\n",
      "05:13\n",
      "05:14\n",
      "05:15\n",
      "05:16\n",
      "05:17\n",
      "05:18\n",
      "05:19\n",
      "05:20\n",
      "05:21\n",
      "05:22\n",
      "05:23\n",
      "05:24\n",
      "05:25\n",
      "05:26\n",
      "05:27\n",
      "05:28\n",
      "05:29\n",
      "05:30\n",
      "05:31\n",
      "05:32\n",
      "05:33\n",
      "05:34\n",
      "05:35\n",
      "05:36\n",
      "05:37\n",
      "05:38\n",
      "05:39\n",
      "05:40\n",
      "05:41\n",
      "05:42\n",
      "05:43\n",
      "05:44\n",
      "05:45\n",
      "05:46\n",
      "05:47\n",
      "05:48\n",
      "05:49\n",
      "05:50\n",
      "05:51\n",
      "05:52\n",
      "05:53\n",
      "05:54\n",
      "05:55\n",
      "05:56\n",
      "05:57\n",
      "05:58\n",
      "05:59\n",
      "06:00\n",
      "06:01\n",
      "06:02\n",
      "06:03\n",
      "06:04\n",
      "06:05\n",
      "06:06\n",
      "06:07\n",
      "06:08\n",
      "06:09\n",
      "06:10\n",
      "06:11\n",
      "06:12\n",
      "06:13\n",
      "06:14\n",
      "06:15\n",
      "06:16\n",
      "06:17\n",
      "06:18\n",
      "06:19\n",
      "06:20\n",
      "06:21\n",
      "06:22\n",
      "06:23\n",
      "06:24\n",
      "06:25\n",
      "06:26\n",
      "06:27\n",
      "06:28\n",
      "06:29\n",
      "06:30\n",
      "06:31\n",
      "06:32\n",
      "06:33\n",
      "06:34\n",
      "06:35\n",
      "06:36\n",
      "06:37\n",
      "06:38\n",
      "06:39\n",
      "06:40\n",
      "06:41\n",
      "06:42\n",
      "06:43\n",
      "06:44\n",
      "06:45\n",
      "06:46\n",
      "06:47\n",
      "06:48\n",
      "06:49\n",
      "06:50\n",
      "06:51\n",
      "06:52\n",
      "06:53\n",
      "06:54\n",
      "06:55\n",
      "06:56\n",
      "06:57\n",
      "06:58\n",
      "06:59\n",
      "07:00\n",
      "07:01\n",
      "07:02\n",
      "07:03\n",
      "07:04\n",
      "07:05\n",
      "07:06\n",
      "07:07\n",
      "07:08\n",
      "07:09\n",
      "07:10\n",
      "07:11\n",
      "07:12\n",
      "07:13\n",
      "07:14\n",
      "07:15\n",
      "07:16\n",
      "07:17\n",
      "07:18\n",
      "07:19\n",
      "07:20\n",
      "07:21\n",
      "07:22\n",
      "07:23\n",
      "07:24\n",
      "07:25\n",
      "07:26\n",
      "07:27\n",
      "07:28\n",
      "07:29\n",
      "07:30\n",
      "07:31\n",
      "07:32\n",
      "07:33\n",
      "07:34\n",
      "07:35\n",
      "07:36\n",
      "07:37\n",
      "07:38\n",
      "07:39\n",
      "07:40\n",
      "07:41\n",
      "07:42\n",
      "07:43\n",
      "07:44\n",
      "07:45\n",
      "07:46\n",
      "07:47\n",
      "07:48\n",
      "07:49\n",
      "07:50\n",
      "07:51\n",
      "07:52\n",
      "07:53\n",
      "07:54\n",
      "07:55\n",
      "07:56\n",
      "07:57\n",
      "07:58\n",
      "07:59\n",
      "08:00\n",
      "08:01\n",
      "08:02\n",
      "08:03\n",
      "08:04\n",
      "08:05\n",
      "08:06\n",
      "08:07\n",
      "08:08\n",
      "08:09\n",
      "08:10\n",
      "08:11\n",
      "08:12\n",
      "08:13\n",
      "08:14\n",
      "08:15\n",
      "08:16\n",
      "08:17\n",
      "08:18\n",
      "08:19\n",
      "08:20\n",
      "08:21\n",
      "08:22\n",
      "08:23\n",
      "08:24\n",
      "08:25\n",
      "08:26\n",
      "08:27\n",
      "08:28\n",
      "08:29\n",
      "08:30\n",
      "08:31\n",
      "08:32\n",
      "08:33\n",
      "08:34\n",
      "08:35\n",
      "08:36\n",
      "08:37\n",
      "08:38\n",
      "08:39\n",
      "08:40\n",
      "08:41\n",
      "08:42\n",
      "08:43\n",
      "08:44\n",
      "08:45\n",
      "08:46\n",
      "08:47\n",
      "08:48\n",
      "08:49\n",
      "08:50\n",
      "08:51\n",
      "08:52\n",
      "08:53\n",
      "08:54\n",
      "08:55\n",
      "08:56\n",
      "08:57\n",
      "08:58\n",
      "08:59\n",
      "09:00\n",
      "09:01\n",
      "09:02\n",
      "09:03\n",
      "09:04\n",
      "09:05\n",
      "09:06\n",
      "09:07\n",
      "09:08\n",
      "09:09\n",
      "09:10\n",
      "09:11\n",
      "09:12\n",
      "09:13\n",
      "09:14\n",
      "09:15\n",
      "09:16\n",
      "09:17\n",
      "09:18\n",
      "09:19\n",
      "09:20\n",
      "09:21\n",
      "09:22\n",
      "09:23\n",
      "09:24\n",
      "09:25\n",
      "09:26\n",
      "09:27\n",
      "09:28\n",
      "09:29\n",
      "09:30\n",
      "09:31\n",
      "09:32\n",
      "09:33\n",
      "09:34\n",
      "09:35\n",
      "09:36\n",
      "09:37\n",
      "09:38\n",
      "09:39\n",
      "09:40\n",
      "09:41\n",
      "09:42\n",
      "09:43\n",
      "09:44\n",
      "09:45\n",
      "09:46\n",
      "09:47\n",
      "09:48\n",
      "09:49\n",
      "09:50\n",
      "09:51\n",
      "09:52\n",
      "09:53\n",
      "09:54\n",
      "09:55\n",
      "09:56\n",
      "09:57\n",
      "09:58\n",
      "09:59\n",
      "10:00\n",
      "10:01\n",
      "10:02\n",
      "10:03\n",
      "10:04\n",
      "10:05\n",
      "10:06\n",
      "10:07\n",
      "10:08\n",
      "10:09\n",
      "10:10\n",
      "10:11\n",
      "10:12\n",
      "10:13\n",
      "10:14\n",
      "10:15\n",
      "10:16\n",
      "10:17\n",
      "10:18\n",
      "10:19\n",
      "10:20\n",
      "10:21\n",
      "10:22\n",
      "10:23\n",
      "10:24\n",
      "10:25\n",
      "10:26\n",
      "10:27\n",
      "10:28\n",
      "10:29\n",
      "10:30\n",
      "10:31\n",
      "10:32\n",
      "10:33\n",
      "10:34\n",
      "10:35\n",
      "10:36\n",
      "10:37\n",
      "10:38\n",
      "10:39\n",
      "10:40\n",
      "10:41\n",
      "10:42\n",
      "10:43\n",
      "10:44\n",
      "10:45\n",
      "10:46\n",
      "10:47\n",
      "10:48\n",
      "10:49\n",
      "10:50\n",
      "10:51\n",
      "10:52\n",
      "10:53\n",
      "10:54\n",
      "10:55\n",
      "10:56\n",
      "10:57\n",
      "10:58\n",
      "10:59\n",
      "11:00\n",
      "11:01\n",
      "11:02\n",
      "11:03\n",
      "11:04\n",
      "11:05\n",
      "11:06\n",
      "11:07\n",
      "11:08\n",
      "11:09\n",
      "11:10\n",
      "11:11\n",
      "11:12\n",
      "11:13\n",
      "11:14\n",
      "11:15\n",
      "11:16\n",
      "11:17\n",
      "11:18\n",
      "11:19\n",
      "11:20\n",
      "11:21\n",
      "11:22\n",
      "11:23\n",
      "11:24\n",
      "11:25\n",
      "11:26\n",
      "11:27\n",
      "11:28\n",
      "11:29\n",
      "11:30\n",
      "11:31\n",
      "11:32\n",
      "11:33\n",
      "11:34\n",
      "11:35\n",
      "11:36\n",
      "11:37\n",
      "11:38\n",
      "11:39\n",
      "11:40\n",
      "11:41\n",
      "11:42\n",
      "11:43\n",
      "11:44\n",
      "11:45\n",
      "11:46\n",
      "11:47\n",
      "11:48\n",
      "11:49\n",
      "11:50\n",
      "11:51\n",
      "11:52\n",
      "11:53\n",
      "11:54\n",
      "11:55\n",
      "11:56\n",
      "11:57\n",
      "11:58\n",
      "11:59\n",
      "12:00\n",
      "12:01\n",
      "12:02\n",
      "12:03\n",
      "12:04\n",
      "12:05\n",
      "12:06\n",
      "12:07\n",
      "12:08\n",
      "12:09\n",
      "12:10\n",
      "12:11\n",
      "12:12\n",
      "12:13\n",
      "12:14\n",
      "12:15\n",
      "12:16\n",
      "12:17\n",
      "12:18\n",
      "12:19\n",
      "12:20\n",
      "12:21\n",
      "12:22\n",
      "12:23\n",
      "12:24\n",
      "12:25\n",
      "12:26\n",
      "12:27\n",
      "12:28\n",
      "12:29\n",
      "12:30\n",
      "12:31\n",
      "12:32\n",
      "12:33\n",
      "12:34\n",
      "12:35\n",
      "12:36\n",
      "12:37\n",
      "12:38\n",
      "12:39\n",
      "12:40\n",
      "12:41\n",
      "12:42\n",
      "12:43\n",
      "12:44\n",
      "12:45\n",
      "12:46\n",
      "12:47\n",
      "12:48\n",
      "12:49\n",
      "12:50\n",
      "12:51\n",
      "12:52\n",
      "12:53\n",
      "12:54\n",
      "12:55\n",
      "12:56\n",
      "12:57\n",
      "12:58\n",
      "12:59\n",
      "13:00\n",
      "13:01\n",
      "13:02\n",
      "13:03\n",
      "13:04\n",
      "13:05\n",
      "13:06\n",
      "13:07\n",
      "13:08\n",
      "13:09\n",
      "13:10\n",
      "13:11\n",
      "13:12\n",
      "13:13\n",
      "13:14\n",
      "13:15\n",
      "13:16\n",
      "13:17\n",
      "13:18\n",
      "13:19\n",
      "13:20\n",
      "13:21\n",
      "13:22\n",
      "13:23\n",
      "13:24\n",
      "13:25\n",
      "13:26\n",
      "13:27\n",
      "13:28\n",
      "13:29\n",
      "13:30\n",
      "13:31\n",
      "13:32\n",
      "13:33\n",
      "13:34\n",
      "13:35\n",
      "13:36\n",
      "13:37\n",
      "13:38\n",
      "13:39\n",
      "13:40\n",
      "13:41\n",
      "13:42\n",
      "13:43\n",
      "13:44\n",
      "13:45\n",
      "13:46\n",
      "13:47\n",
      "13:48\n",
      "13:49\n",
      "13:50\n",
      "13:51\n",
      "13:52\n",
      "13:53\n",
      "13:54\n",
      "13:55\n",
      "13:56\n",
      "13:57\n",
      "13:58\n",
      "13:59\n",
      "14:00\n",
      "14:01\n",
      "14:02\n",
      "14:03\n",
      "14:04\n",
      "14:05\n",
      "14:06\n",
      "14:07\n",
      "14:08\n",
      "14:09\n",
      "14:10\n",
      "14:11\n",
      "14:12\n",
      "14:13\n",
      "14:14\n",
      "14:15\n",
      "14:16\n",
      "14:17\n",
      "14:18\n",
      "14:19\n",
      "14:20\n",
      "14:21\n",
      "14:22\n",
      "14:23\n",
      "14:24\n",
      "14:25\n",
      "14:26\n",
      "14:27\n",
      "14:28\n",
      "14:29\n",
      "14:30\n",
      "14:31\n",
      "14:32\n",
      "14:33\n",
      "14:34\n",
      "14:35\n",
      "14:36\n",
      "14:37\n",
      "14:38\n",
      "14:39\n",
      "14:40\n",
      "14:41\n",
      "14:42\n",
      "14:43\n",
      "14:44\n",
      "14:45\n",
      "14:46\n",
      "14:47\n",
      "14:48\n",
      "14:49\n",
      "14:50\n",
      "14:51\n",
      "14:52\n",
      "14:53\n",
      "14:54\n",
      "14:55\n",
      "14:56\n",
      "14:57\n",
      "14:58\n",
      "14:59\n",
      "15:00\n",
      "15:01\n",
      "15:02\n",
      "15:03\n",
      "15:04\n",
      "15:05\n",
      "15:06\n",
      "15:07\n",
      "15:08\n",
      "15:09\n",
      "15:10\n",
      "15:11\n",
      "15:12\n",
      "15:13\n",
      "15:14\n",
      "15:15\n",
      "15:16\n",
      "15:17\n",
      "15:18\n",
      "15:19\n",
      "15:20\n",
      "15:21\n",
      "15:22\n",
      "15:23\n",
      "15:24\n",
      "15:25\n",
      "15:26\n",
      "15:27\n",
      "15:28\n",
      "15:29\n",
      "15:30\n",
      "15:31\n",
      "15:32\n",
      "15:33\n",
      "15:34\n",
      "15:35\n",
      "15:36\n",
      "15:37\n",
      "15:38\n",
      "15:39\n",
      "15:40\n",
      "15:41\n",
      "15:42\n",
      "15:43\n",
      "15:44\n",
      "15:45\n",
      "15:46\n",
      "15:47\n",
      "15:48\n",
      "15:49\n",
      "15:50\n",
      "15:51\n",
      "15:52\n",
      "15:53\n",
      "15:54\n",
      "15:55\n",
      "15:56\n",
      "15:57\n",
      "15:58\n",
      "15:59\n",
      "16:00\n",
      "16:01\n",
      "16:02\n",
      "16:03\n",
      "16:04\n",
      "16:05\n",
      "16:06\n",
      "16:07\n",
      "16:08\n",
      "16:09\n",
      "16:10\n",
      "16:11\n",
      "16:12\n",
      "16:13\n",
      "16:14\n",
      "16:15\n",
      "16:16\n",
      "16:17\n",
      "16:18\n",
      "16:19\n",
      "16:20\n",
      "16:21\n",
      "16:22\n",
      "16:23\n",
      "16:24\n",
      "16:25\n",
      "16:26\n",
      "16:27\n",
      "16:28\n",
      "16:29\n",
      "16:30\n",
      "16:31\n",
      "16:32\n",
      "16:33\n",
      "16:34\n",
      "16:35\n",
      "16:36\n",
      "16:37\n",
      "16:38\n",
      "16:39\n",
      "16:40\n",
      "16:41\n",
      "16:42\n",
      "16:43\n",
      "16:44\n",
      "16:45\n",
      "16:46\n",
      "16:47\n",
      "16:48\n",
      "16:49\n",
      "16:50\n",
      "16:51\n",
      "16:52\n",
      "16:53\n",
      "16:54\n",
      "16:55\n",
      "16:56\n",
      "16:57\n",
      "16:58\n",
      "16:59\n",
      "17:00\n",
      "17:01\n",
      "17:02\n",
      "17:03\n",
      "17:04\n",
      "17:05\n",
      "17:06\n",
      "17:07\n",
      "17:08\n",
      "17:09\n",
      "17:10\n",
      "17:11\n",
      "17:12\n",
      "17:13\n",
      "17:14\n",
      "17:15\n",
      "17:16\n",
      "17:17\n",
      "17:18\n",
      "17:19\n",
      "17:20\n",
      "17:21\n",
      "17:22\n",
      "17:23\n",
      "17:24\n",
      "17:25\n",
      "17:26\n",
      "17:27\n",
      "17:28\n",
      "17:29\n",
      "17:30\n",
      "17:31\n",
      "17:32\n",
      "17:33\n",
      "17:34\n",
      "17:35\n",
      "17:36\n",
      "17:37\n",
      "17:38\n",
      "17:39\n",
      "17:40\n",
      "17:41\n",
      "17:42\n",
      "17:43\n",
      "17:44\n",
      "17:45\n",
      "17:46\n",
      "17:47\n",
      "17:48\n",
      "17:49\n",
      "17:50\n",
      "17:51\n",
      "17:52\n",
      "17:53\n",
      "17:54\n",
      "17:55\n",
      "17:56\n",
      "17:57\n",
      "17:58\n",
      "17:59\n",
      "18:00\n",
      "18:01\n",
      "18:02\n",
      "18:03\n",
      "18:04\n",
      "18:05\n",
      "18:06\n",
      "18:07\n",
      "18:08\n",
      "18:09\n",
      "18:10\n",
      "18:11\n",
      "18:12\n",
      "18:13\n",
      "18:14\n",
      "18:15\n",
      "18:16\n",
      "18:17\n",
      "18:18\n",
      "18:19\n",
      "18:20\n",
      "18:21\n",
      "18:22\n",
      "18:23\n",
      "18:24\n",
      "18:25\n",
      "18:26\n",
      "18:27\n",
      "18:28\n",
      "18:29\n",
      "18:30\n",
      "18:31\n",
      "18:32\n",
      "18:33\n",
      "18:34\n",
      "18:35\n",
      "18:36\n",
      "18:37\n",
      "18:38\n",
      "18:39\n",
      "18:40\n",
      "18:41\n",
      "18:42\n",
      "18:43\n",
      "18:44\n",
      "18:45\n",
      "18:46\n",
      "18:47\n",
      "18:48\n",
      "18:49\n",
      "18:50\n",
      "18:51\n",
      "18:52\n",
      "18:53\n",
      "18:54\n",
      "18:55\n",
      "18:56\n",
      "18:57\n",
      "18:58\n",
      "18:59\n",
      "19:00\n",
      "19:01\n",
      "19:02\n",
      "19:03\n",
      "19:04\n",
      "19:05\n",
      "19:06\n",
      "19:07\n",
      "19:08\n",
      "19:09\n",
      "19:10\n",
      "19:11\n",
      "19:12\n",
      "19:13\n",
      "19:14\n",
      "19:15\n",
      "19:16\n",
      "19:17\n",
      "19:18\n",
      "19:19\n",
      "19:20\n",
      "19:21\n",
      "19:22\n",
      "19:23\n",
      "19:24\n",
      "19:25\n",
      "19:26\n",
      "19:27\n",
      "19:28\n",
      "19:29\n",
      "19:30\n",
      "19:31\n",
      "19:32\n",
      "19:33\n",
      "19:34\n",
      "19:35\n",
      "19:36\n",
      "19:37\n",
      "19:38\n",
      "19:39\n",
      "19:40\n",
      "19:41\n",
      "19:42\n",
      "19:43\n",
      "19:44\n",
      "19:45\n",
      "19:46\n",
      "19:47\n",
      "19:48\n",
      "19:49\n",
      "19:50\n",
      "19:51\n",
      "19:52\n",
      "19:53\n",
      "19:54\n",
      "19:55\n",
      "19:56\n",
      "19:57\n",
      "19:58\n",
      "19:59\n",
      "20:00\n",
      "20:01\n",
      "20:02\n",
      "20:03\n",
      "20:04\n",
      "20:05\n",
      "20:06\n",
      "20:07\n",
      "20:08\n",
      "20:09\n",
      "20:10\n",
      "20:11\n",
      "20:12\n",
      "20:13\n",
      "20:14\n",
      "20:15\n",
      "20:16\n",
      "20:17\n",
      "20:18\n",
      "20:19\n",
      "20:20\n",
      "20:21\n",
      "20:22\n",
      "20:23\n",
      "20:24\n",
      "20:25\n",
      "20:26\n",
      "20:27\n",
      "20:28\n",
      "20:29\n",
      "20:30\n",
      "20:31\n",
      "20:32\n",
      "20:33\n",
      "20:34\n",
      "20:35\n",
      "20:36\n",
      "20:37\n",
      "20:38\n",
      "20:39\n",
      "20:40\n",
      "20:41\n",
      "20:42\n",
      "20:43\n",
      "20:44\n",
      "20:45\n",
      "20:46\n",
      "20:47\n",
      "20:48\n",
      "20:49\n",
      "20:50\n",
      "20:51\n",
      "20:52\n",
      "20:53\n",
      "20:54\n",
      "20:55\n",
      "20:56\n",
      "20:57\n",
      "20:58\n",
      "20:59\n",
      "21:00\n",
      "21:01\n",
      "21:02\n",
      "21:03\n",
      "21:04\n",
      "21:05\n",
      "21:06\n",
      "21:07\n",
      "21:08\n",
      "21:09\n",
      "21:10\n",
      "21:11\n",
      "21:12\n",
      "21:13\n",
      "21:14\n",
      "21:15\n",
      "21:16\n",
      "21:17\n",
      "21:18\n",
      "21:19\n",
      "21:20\n",
      "21:21\n",
      "21:22\n",
      "21:23\n",
      "21:24\n",
      "21:25\n",
      "21:26\n",
      "21:27\n",
      "21:28\n",
      "21:29\n",
      "21:30\n",
      "21:31\n",
      "21:32\n",
      "21:33\n",
      "21:34\n",
      "21:35\n",
      "21:36\n",
      "21:37\n",
      "21:38\n",
      "21:39\n",
      "21:40\n",
      "21:41\n",
      "21:42\n",
      "21:43\n",
      "21:44\n",
      "21:45\n",
      "21:46\n",
      "21:47\n",
      "21:48\n",
      "21:49\n",
      "21:50\n",
      "21:51\n",
      "21:52\n",
      "21:53\n",
      "21:54\n",
      "21:55\n",
      "21:56\n",
      "21:57\n",
      "21:58\n",
      "21:59\n",
      "22:00\n",
      "22:01\n",
      "22:02\n",
      "22:03\n",
      "22:04\n",
      "22:05\n",
      "22:06\n",
      "22:07\n",
      "22:08\n",
      "22:09\n",
      "22:10\n",
      "22:11\n",
      "22:12\n",
      "22:13\n",
      "22:14\n",
      "22:15\n",
      "22:16\n",
      "22:17\n",
      "22:18\n",
      "22:19\n",
      "22:20\n",
      "22:21\n",
      "22:22\n",
      "22:23\n",
      "22:24\n",
      "22:25\n",
      "22:26\n",
      "22:27\n",
      "22:28\n",
      "22:29\n",
      "22:30\n",
      "22:31\n",
      "22:32\n",
      "22:33\n",
      "22:34\n",
      "22:35\n",
      "22:36\n",
      "22:37\n",
      "22:38\n",
      "22:39\n",
      "22:40\n",
      "22:41\n",
      "22:42\n",
      "22:43\n",
      "22:44\n",
      "22:45\n",
      "22:46\n",
      "22:47\n",
      "22:48\n",
      "22:49\n",
      "22:50\n",
      "22:51\n",
      "22:52\n",
      "22:53\n",
      "22:54\n",
      "22:55\n",
      "22:56\n",
      "22:57\n",
      "22:58\n",
      "22:59\n",
      "23:00\n",
      "23:01\n",
      "23:02\n",
      "23:03\n",
      "23:04\n",
      "23:05\n",
      "23:06\n",
      "23:07\n",
      "23:08\n",
      "23:09\n",
      "23:10\n",
      "23:11\n",
      "23:12\n",
      "23:13\n",
      "23:14\n",
      "23:15\n",
      "23:16\n",
      "23:17\n",
      "23:18\n",
      "23:19\n",
      "23:20\n",
      "23:21\n",
      "23:22\n",
      "23:23\n",
      "23:24\n",
      "23:25\n",
      "23:26\n",
      "23:27\n",
      "23:28\n",
      "23:29\n",
      "23:30\n",
      "23:31\n",
      "23:32\n",
      "23:33\n",
      "23:34\n",
      "23:35\n",
      "23:36\n",
      "23:37\n",
      "23:38\n",
      "23:39\n",
      "23:40\n",
      "23:41\n",
      "23:42\n",
      "23:43\n",
      "23:44\n",
      "23:45\n",
      "23:46\n",
      "23:47\n",
      "23:48\n",
      "23:49\n",
      "23:50\n",
      "23:51\n",
      "23:52\n",
      "23:53\n",
      "23:54\n",
      "23:55\n",
      "23:56\n",
      "23:57\n",
      "23:58\n",
      "23:59\n"
     ]
    }
   ],
   "source": [
    "for hour in range(0,24):\n",
    "    for minute in range(0,60):\n",
    "        print(f\"{hour:02d}:{minute:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yash vs yash\n",
      "yash vs lakshy\n",
      "yash vs Ishaan\n",
      "yash vs Gaurav\n",
      "lakshy vs yash\n",
      "lakshy vs lakshy\n",
      "lakshy vs Ishaan\n",
      "lakshy vs Gaurav\n",
      "Ishaan vs yash\n",
      "Ishaan vs lakshy\n",
      "Ishaan vs Ishaan\n",
      "Ishaan vs Gaurav\n",
      "Gaurav vs yash\n",
      "Gaurav vs lakshy\n",
      "Gaurav vs Ishaan\n",
      "Gaurav vs Gaurav\n"
     ]
    }
   ],
   "source": [
    "players = [\"yash\", \"lakshy\", \"Ishaan\", \"Gaurav\"]\n",
    "\n",
    "for player1 in players:\n",
    "    for player2 in players:\n",
    "        print(f\"{player1} vs {player2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f04be",
   "metadata": {},
   "source": [
    "## torch.nn.module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aceb88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output without nn.module: tensor([[-8.3074]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#implementation without nn.module\n",
    "import torch\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "w1 = torch.randn(input_size, hidden_size, requires_grad = True)\n",
    "b1 = torch.randn(hidden_size, requires_grad = True)\n",
    "w2 = torch.randn(hidden_size, output_size, requires_grad= True)\n",
    "b2 = torch.randn(output_size, requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    \n",
    "    hidden = torch.matmul(x, w1) + b1\n",
    "    hidden_activation = torch.relu(hidden)\n",
    "\n",
    "    output = torch.matmul(hidden_activation, w2) + b2\n",
    "    return output\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0]]) #batch of one input wiht 2 features \n",
    "y = forward(x)\n",
    "print(\"output without nn.module:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "290fc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output with the torch.nn:  tensor([[0.1165]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implementation with torchh.nn.module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "class SimpleNN(nn.Module):              # inheriting the nn.Module\n",
    "    def __init__(self, input_size, hidden_size, output_size):  # constructor\n",
    "        super(SimpleNN, self).__init__()                       # envoking the parent class(nn.Module) constructor with the help of super()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.hidden(x)\n",
    "        hidden_activation = torch.relu(hidden)\n",
    "        output = self.output(hidden_activation)\n",
    "        return output\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "x = torch.tensor([[1.0,2.0]])\n",
    "y = model(x)\n",
    "print(\"output with the torch.nn: \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffe574",
   "metadata": {},
   "source": [
    "1.> super(SimpleNN, self).__init__() : this is use for initiallization of the nn.module(calls the __init__ method of the parent class(nn.Module))\n",
    "\n",
    "2.> super() is an inbuilt function that provides a way to access methods and properties of a parent or sibling class from within a child (subclass)\n",
    "\n",
    "3.> self (constructor) : It represnets the instance(object) of the class being used. it acts as a pointer or refreence to the current object that is created from the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee0183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common layers in torch.nn ##\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(). __init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575f5311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5838],\n",
       "        [0.4856],\n",
       "        [0.5555],\n",
       "        [0.5633],\n",
       "        [0.4824],\n",
       "        [0.4630],\n",
       "        [0.5721],\n",
       "        [0.5589],\n",
       "        [0.5632],\n",
       "        [0.4683]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataset with 10 samples and 5 features\n",
    "features = torch.randn(10, 5)\n",
    "#create model instance\n",
    "model = Model(features.shape[1])\n",
    "# forward pass\n",
    "# model.forward(features)\n",
    "model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a5eaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1246], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight\n",
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e98415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "Linear: 1-1                            [10, 1]                   6\n",
       "Sigmoid: 1-2                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary \n",
    "\n",
    "summary(model, input_size=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49486ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4911],\n",
       "        [0.3792],\n",
       "        [0.4684],\n",
       "        [0.5009],\n",
       "        [0.4806],\n",
       "        [0.5012],\n",
       "        [0.4070],\n",
       "        [0.4893],\n",
       "        [0.4675],\n",
       "        [0.4991]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_features, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.linear1(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out \n",
    "    \n",
    "# create dataset with 10 samples an 5 features\n",
    "features = torch.randn(10, 5)\n",
    "#create model instance \n",
    "model = Model(features.shape[1])\n",
    "# forward pass\n",
    "model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac032ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2807, -0.0019, -0.1647,  0.0266,  0.1987],\n",
       "        [ 0.0672, -0.1692, -0.2085,  0.0874, -0.1970],\n",
       "        [ 0.3311, -0.1621,  0.1110,  0.0038, -0.2571]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77e53e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0522, -0.5020, -0.5402]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear2.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17afd721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3340, 0.0607, 0.1546], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a0fa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0368], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "851fa1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "Linear: 1-1                            [10, 3]                   18\n",
       "ReLU: 1-2                              [10, 3]                   --\n",
       "Linear: 1-3                            [10, 1]                   4\n",
       "Sigmoid: 1-4                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 22\n",
       "Trainable params: 22\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary \n",
    "summary(model, input_size=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5977],\n",
       "        [0.6049],\n",
       "        [0.5956],\n",
       "        [0.4674],\n",
       "        [0.6049],\n",
       "        [0.6049],\n",
       "        [0.5850],\n",
       "        [0.6049],\n",
       "        [0.5409],\n",
       "        [0.5783]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another approach by using sequentiall container \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "        return out \n",
    "# create dataset with 10 sample and 5 features\n",
    "features = torch.randn(10, 5)\n",
    "# create model instance \n",
    "model = Model(features.shape[1])\n",
    "# forward pass\n",
    "model(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cab21be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0607, -0.1247,  0.1379,  0.2983,  0.3690],\n",
       "        [-0.1642, -0.1074, -0.0650, -0.1598,  0.1487],\n",
       "        [ 0.1833, -0.0237,  0.0553, -0.3360,  0.2641]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70618915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1783, -0.2730, -0.4188], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc5f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1337, -0.3417, -0.5530]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a71355bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4258], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd8531",
   "metadata": {},
   "source": [
    "## nn.Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697e910",
   "metadata": {},
   "source": [
    "* torch.nn.Parameter is a subclass of torch.Tensor, designed for holding parameters in the model that should be considered during the training.\n",
    "\n",
    "* when a tensor is wrapped with torch.nn.Parameter, it automatically becomes the part of the model's parameters, and thus it will be updated when backpropagation is applied during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bacba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLinear, self).__init__()\n",
    "        # define weights and bias parameters\n",
    "        self.weights = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #implementing a forward pass\n",
    "        return torch.matmul(x, self.weights.t()) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bef32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " tensor([[-0.5461, -0.9793, -0.6256, -0.6881],\n",
      "        [ 0.2745, -1.0656,  0.6502,  1.7235],\n",
      "        [ 0.9593,  0.1801, -0.0964, -1.7881]])\n",
      "output: \n",
      " tensor([[-2.9095, -1.0992],\n",
      "        [ 2.9338,  1.4810],\n",
      "        [-4.1624, -1.1269]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4) # batch_size = 3, in_features = 4\n",
    "\n",
    "linear = MyLinear(in_features=4, out_features=2)\n",
    "\n",
    "output = linear(x)\n",
    "\n",
    "print(\"Input: \\n\", x)\n",
    "print(\"output: \\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020333d",
   "metadata": {},
   "source": [
    "SLICING OF TENSOR IN PYTHON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing 1D tensor using -> [start, stop, step]\n",
    "\n",
    "import torch \n",
    "x = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "sliced_x = x[2:7:2]\n",
    "print(sliced_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b58d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing Multi dimensional tensor \n",
    "# Applying slicing to each dimension seprated by commmas\n",
    "\n",
    "import torch\n",
    "y = torch.tensor([[1,2,3],\n",
    "                  [4,5,6],\n",
    "                  [7,8,9]])\n",
    "\n",
    "# select rows from index 0 upto (but not including) 2, and all columns \n",
    "sliced_y = y[0:2,:]\n",
    "print(sliced_y)\n",
    "\n",
    "# select all rows, and columns form index 1 upto (but not including) 3\n",
    "sliced_y_cols = y[0:,1:3]\n",
    "print(sliced_y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231b257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.triu(torch.ones(6,6), diagonal=1)\n",
    "print(z)\n",
    "z[3:4, :4] # array[start_row:end_row, start_column:end_column]\n",
    "# 3:4 -> specifies of rows to select\n",
    "# start index (3), end index (4)(go up to but not include row index 4)\n",
    "# column slicing [:4] -> specifes the range of columns to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57da256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([0., 1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "##### torch.arange(....) ######\n",
    "import torch\n",
    "\n",
    "tensor1 = torch.arange(5)\n",
    "print(tensor1)\n",
    "\n",
    "# torch.arange(start=0, end=None, step=1, dtype=None, device=None)\n",
    "\n",
    "tensor2 = torch.arange(1, 10, 2)\n",
    "print(tensor2)\n",
    "\n",
    "tensor3 = torch.arange(5, dtype=torch.float32)\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1572491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 7,  8,  9, 10]])\n",
      "torch.Size([1, 3, 4])\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 7,  8,  9, 10]]])\n",
      "torch.Size([3, 1, 4])\n",
      "tensor([[[ 1,  2,  3,  4]],\n",
      "\n",
      "        [[ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 7,  8,  9, 10]]])\n"
     ]
    }
   ],
   "source": [
    "##### torch.unsqueeze() ######\n",
    "\n",
    "x = torch.tensor([[1,2,3,4], [4,5,6,7], [7,8,9,10]])\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "y = x.unsqueeze(0) # add a new dimension at index 0\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "z = x.unsqueeze(1) \n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02a8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ffc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e4e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
